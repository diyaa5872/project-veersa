{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56d507a0-96eb-4791-bea7-31dc05b74994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, length, upper, lower, trim, lit\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# Map from config data_type to Spark type\n",
    "DATA_TYPE_MAP = {\n",
    "    \"STRING\": StringType(),\n",
    "    \"INT\": IntegerType()\n",
    "}\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Raw customer data after data quality checks and validations\",\n",
    "    table_properties={\"quality\": \"bronze_stage\"}\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_customer_id\", \"customerId IS NOT NULL AND LENGTH(customerId) > 0\")\n",
    "@dlt.expect_or_drop(\"valid_customer_unique_id\", \"ucn IS NOT NULL AND LENGTH(ucn) > 0\")\n",
    "@dlt.expect_or_drop(\"valid_zip_code\", \"zip_prefix RLIKE '^[0-9]{4,5}$'\")  # supports 4 or 5 digit zip prefix\n",
    "@dlt.expect_or_drop(\"valid_state\", \"state IS NOT NULL AND LENGTH(state) = 2\")\n",
    "@dlt.expect_or_drop(\"valid_city\", \"city IS NOT NULL AND LENGTH(city) > 0\")\n",
    "def bronze_stage():\n",
    "    # 1. Read streaming source\n",
    "    raw_df = dlt.read_stream(\"bronze_raw\")\n",
    "\n",
    "    # 2. Read configuration table\n",
    "    config_df = spark.table(\"`unity-veersa`.config_tables.config_raw_to_stage\").filter(col(\"is_active\") == True)\n",
    "\n",
    "    # 3. Normalize raw data (city lower, state upper)\n",
    "    normalized_df = (\n",
    "        raw_df\n",
    "        .withColumn(\"city\", lower(trim(col(\"city\"))))\n",
    "        .withColumn(\"state\", upper(trim(col(\"state\"))))\n",
    "    )\n",
    "\n",
    "    # 4. Dynamically select and alias columns based on config\n",
    "    selected_cols = []\n",
    "    for row in config_df.collect():\n",
    "        src_col = row[\"column_name\"]      # e.g., 'customerId', 'ucn'\n",
    "        alias = row[\"alias_name\"]         # output alias\n",
    "        dtype = row[\"data_type\"]          # STRING/INT\n",
    "        default_val = row[\"default_value\"]\n",
    "\n",
    "        # If source column exists, use it; else fallback to default\n",
    "        if src_col in normalized_df.columns:\n",
    "            col_expr = col(src_col)\n",
    "        elif default_val is not None:\n",
    "            col_expr = lit(default_val)\n",
    "        else:\n",
    "            col_expr = lit(None)\n",
    "\n",
    "        # Cast to required data type\n",
    "        if dtype and dtype.upper() in DATA_TYPE_MAP:\n",
    "            col_expr = col_expr.cast(DATA_TYPE_MAP[dtype.upper()])\n",
    "\n",
    "        selected_cols.append(col_expr.alias(alias))\n",
    "\n",
    "    # Select columns as per config\n",
    "    transformed_df = normalized_df.select(*selected_cols)\n",
    "\n",
    "    return transformed_df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "raw_to_stage",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
